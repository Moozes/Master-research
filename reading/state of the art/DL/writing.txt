[ref12]
Skin cancer detection: Applying a deep learning based model driven architecture in the cloud for classifying dermal cell images
in this paper the researchers are presenting a model driven approach to develop deep learning algorithms for detecting skin cancer by using a tool called DLS (deep learning studio) which is a software that allows you to build deep learning algorithms without being a specialist in programming languages, it presents a simple drag and drop interface for building models it also commes with desktop / cloud versions and community / enterprise editions with multi-GPU trainning and the possibility to obtain the code of the model, download the model and host it as a REST API (Representational state transfer Application programming interface), the interface dashboard is shown in figure [dls.png]

the advantage of this non programatic approach is for researchers and practitionners to be able to create and test there own models without the need for prior programming knowledge


and then they procede using this tool DLS to show it efficacy and ease of use, they have built and tested 5 models using famous architectures squeeznet, densenet, and inception v3
with model1 aquiring an AUC of 99.77% 


[ref2]
in this work the researchers propose a new idea, which is the use  of clinical information in addition to the image dataset and the study of this addition's effect on the deep learning model's performance

dataset
Dermatological Assistance Program (PAD) dataset at the Federal University of Espírito Santo (UFES), mobile application (used by doctors and students) to build a new dataset  

to collect images of the lesion, there clinical diagnosis and 8 clinical information based on common questions that dermotologists ask
    age
    part of the body where the lesion is located,
    if the lesion itches,
    bleeds or has bled,
    hurts, 
    has recently increased, 
    has changed its pattern, 
    and if it has an elevation

a total of 1612 image of 6 lesions

because the image dataset is imbalanced they used multiple strategies to overcome that such as, transfer learning (refining a pretrained model on there dataset) , data augmentation, horizontal and vetical rotations, adjusting brightness...etc, and for the clinical data they used one-hot encoding (converting categorical data to augment the performance) which transformed the 8 features collected to an array of 28 values 

trainning
    they used 4 CNN architectures VGGNet-13/19-bn, ResNet-50/101, MobileNet, GoogleNet
    now a problem arised when trying to combine (by concatenation) clinical data with image features extracted by the CNN feature extractor because image features are far more great in size then clinical data, this imbalance is not good for the trainning and classification because the effect of image features will be greater then the clinical data, that is why they  they implimented an NN feature reducer on the extracted image features before combining it with the clinical data as shown in figure [clinical-image.png] and the classifier is another neural network that assigns the probabilities for each skin lesion

testing the effect of adding clinical data
    they executed 2 scenarios for that, 1 using models trained only with images, 2 using models trained with images + clinical data then they calculated multiple  performance metrics accuracy , balanced accuracy , weighted precision , weighted recall , weighted F1 score  and area under the curve and they found almost all models was improved by 7% in almost all metrics and the best model ResNet-50 presented an AUC >= 95.8% 

conclusion 
    clinical information does make a difference when trainning ML models to classify skin cancer 




[ref10]
An artificial neural network based detection and classification of melanoma skin cancer using hybrid texture features
    dataset ISIC+PH2
    preprocess to remove hair and enhancement
    segmentation
    multiple features extracted
    train, accuracy 97.7%

    in this work they try to combine multiple texture features from famous methods such as  ABCD GLCM and LBP and pass all of these features to an ANN for learning

    dataset
    they prefered to use images captured using a dermatoscope because of their quality over images captured using a phone or normal camera and they have obtained these images by combinnig 2 datasets: ISIC archive dataset (jpg format) and PH2 dataset (a dermoscopic image dataset in BMP format) they formed a unified dataset containning 1940 benign and 1448 malignant lesion images

    preprocessing
    because the images are obtained from various sources, they needed to process them to standardized them in size, shape, format ...etc
    and also to remove noise and enhance image quality using enhancement algorithms such as histogram equalization process that increases image contrast, and to remove body hair from the images using Maximum Gradient Intensity (MGI) algorithm 

    image segmentation
    for better analysis and to remove unwanted parts they segmented the images to keep only the lesion area and for that they used a segmentation method called Otsu's Thresholding 

    feature extraction
    they used ABCD (Asymmetry, Border, Color, Diameter), GLCM (energy, contrast, correlation, homogeneity) and LBP (local binary pattern used for textural analysis) as features to train there neural network

    classification
        a feed-forward neural network with backprobagation mechanism is used with the input layer receiving the extracted features and a hidden layer of 100 neurons and an output layer for the final result (1 is malignant and 0 is benign) with baises and weights initialised randomly, Levenberg-Marquardt trainning and optimization functions are used and while the performance function being Mean Square Error and 2 activation function "tansig" for the hidden layer and "purelin" for the final output
        the structure of the ANN is shown in figure [ann.png]  
    
    evaluation
        for the evaluation of there classifier they calculated accuracy, specificity, sensitivity and precision shown in figure [evaluation.png] where all the mesures are > 97%, and further more they also studied the effect of each feature on the discrimination process between benign and malignant lesion and they found that the minimum sensitivity per single feature is 69%, minimum specificity per single feature is 73 and minimum accuracy per single feature is 71% which goes to show that all the used features are playing an important role in the classification process and lastly they did a comparative evaluation between there work and previous works on the basis of extracted featues which showed that more featues implies higher performance rates, an example of that is the accuracy of previous works using a combination of some but not all features in (ABCD, GLCM, LBP) always presented an accuracy < 97%

    in conclusion the use of hybrid features provided a higher performant model in the detection and classification of benign and malignant melanoma skin cancer 

[ref13]
sonification could solve the problem between primary and professional dermatoscopic images




[ref23]
Interpretable deep learning systems for multi-class segmentation and classification of non-melanoma skin cancer

analyse the whole image with haire and sweat glands ... as a real doctor sees it, no processing to remove any thing

the use of interpretable learning methods
most of dermatopathology work can be done by a machine 
include all tissue in analysis(skin, haire, sweat glands...) ==> interpretability because machine is seeing every thing that a pathologist would see and analyse
whole tissue analysis ==> we can do other stuff such as sutgical margins ...

traditional models outpreform experts but "black boxes" ==> no high stake decisions

we can use interpretability methods such as 
    ike permutation feature importance, 
    Partial Dependence Plots (PDPs), 
    Individual Conditional Expectation (ICE) plots, 
    global surrogate models, 
    Local Interpretable Model-agnostic Explanations (LIME) 
    Shapley Additive Explanations (SHAP) 
to try and explain our model [https://towardsdatascience.com/model-agnostic-methods-for-interpreting-any-machine-learning-model-4f10787ef504] but there is another way which is naturally interpretable models, which can be defined as models that try to solve the problem the way a human would, which means in the case of skin cance analysing the whole tissue and not just cancerous regions of interest 

we can acheive this with semantic segmentation methods

dataset 
    MyLab Pathology provided them with there pre-existing images on non-melanoma cancers, which was taken by a microscope (one image of a cancer is the result of multiple microscopic images concatenated together) for punch, shave and excision biopsies (shave: the surface of the skin is removed with a sharp knife, punch: a round small part of the skin is removed) which meant high resolution images (1px=0.67µm) and each image was annotated by a pathologist to indicate important tissue section in the discrimination process, 

    any imbalance of classes was solved using augmentation (rotation and flipping...etc)

models 
    whole image segmentation
        input: microscopic image [seg-input.png]
        ouput: h x w x 12 (12 probability distribution maps) [seg-output.png]
        the different tissue sections were colored (to train the algorithm, image(RGB) has 12 layer result )
            1 Glands (GLD) 
            2 Inflammation (INF) 
            3 Hair Follicles (FOL) 
            4 Hypodermis (HYP) 
            5 Reticular Dermis (RET) 
            6 Pap-illary Dermis (PAP) 
            7 Epidermis (EPI) 
            8 Keratin (KER) 
            9 Background (BKG) 
            10 BCC (Basal cell carcinoma)
            11 SCC (Squamous cell carcinoma)
            12 IEC (a very early treatable form of skin cancer)
        to be fed to the segmentation model to train on semantic segmentation, this model was created using a combination of U-net-like architecture (U-net: a famous CNN architecture for biomedical image segmentation) and a pretrained headless ResNet50 network. now because of the high resolution of the microscopic images they were fed to the model in parts of 256x256 and 512x512 pixels

    whole image classification

        input: output of segmantation (h x w x [12 images]) was given to classification
        output: 4 classes Healthy, BCC, SCC and IEC

        the output of whole image segmantation (which was a probability distribution for each pixel on the 12 tissue classes) was fed to a CNN to train as a classifier using Adam optimizer and a learning rate of 0.0001, with a ration of 80:10:10 for trainning, validation and testing


results and discussion
    the segmentation model achieved a per-pixel accuracy of 86% and overall class accuracy of 85%, they found that downgrading the images size before trainning to 10 times less increases the accuracy but only by a little bit ~2% which isnt much but this information is still usefull because it means that we can use low resolution images and still get high performant model with less computational power

    the classification model achieved an accuracy of 93.6% over the 4 classes compared to other algorithms trained with the same data such as (Random Forest 87.2%, KNN 80.9%, Single-layer Perceptron 85.1%)


conclusion
    further uses such as surgical, depth, invasion...


